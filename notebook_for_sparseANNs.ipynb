{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# make sure that you have a GPU: this cell should print 'cuda'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building sparse recurrent neural networks (RNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ba73f26d350>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to ensure reproducibility, we should set the torch seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## make the model - a recurrent network with one hidden layer, and a fully connected output layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__() # initialise the nn.Module\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, nonlinearity = 'relu', batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # set initial hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # pass input throught the recurrent layer\n",
    "        output, hidden = self.rnn(x, h0) # output shape (batch_size, seq_length, hidden_size)\n",
    "        # reshape the output so it fits into the fully connected layer (get the last output from the RNN)\n",
    "        output = output[:, -1, :]\n",
    "        # pass it to the linear layer to get the classification\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_size = 28 # the size of the input at each timestep\n",
    "hidden_size = 10000 # how many nodes do we want in the hidden layer?\n",
    "num_layers = 1 # how many hidden recurrent layers \n",
    "num_classes = 10 # this is defined by the dataset, which has 10 classes (digits 0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(28, 10000, batch_first=True)\n",
      "  (fc): Linear(in_features=10000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_ih_l0', Parameter containing:\n",
      "tensor([[ 7.6454e-03,  8.3001e-03, -2.3427e-03,  ..., -4.6101e-03,\n",
      "         -2.8237e-03, -6.0127e-03],\n",
      "        [ 9.4383e-04, -9.8768e-03,  9.0311e-03,  ...,  8.2054e-03,\n",
      "          2.8803e-03,  4.1421e-03],\n",
      "        [ 3.1626e-03, -1.7396e-04,  7.8261e-03,  ..., -6.8171e-03,\n",
      "          5.3058e-03, -4.0420e-03],\n",
      "        ...,\n",
      "        [ 6.5973e-03, -2.2318e-03,  2.7703e-03,  ..., -4.4835e-03,\n",
      "          6.6070e-03, -8.1006e-03],\n",
      "        [ 2.9607e-03,  6.2242e-03,  4.3185e-03,  ..., -4.0194e-04,\n",
      "          2.5944e-05,  9.5441e-03],\n",
      "        [ 1.1639e-03,  6.6203e-03,  4.5857e-03,  ..., -7.5040e-03,\n",
      "          6.9919e-03, -4.0599e-03]], requires_grad=True)), ('weight_hh_l0', Parameter containing:\n",
      "tensor([[ 0.0070,  0.0043, -0.0034,  ...,  0.0003, -0.0029,  0.0076],\n",
      "        [ 0.0007, -0.0073, -0.0014,  ...,  0.0099, -0.0019, -0.0050],\n",
      "        [ 0.0044,  0.0034,  0.0088,  ..., -0.0020,  0.0070, -0.0095],\n",
      "        ...,\n",
      "        [ 0.0080, -0.0074, -0.0059,  ...,  0.0088, -0.0020,  0.0023],\n",
      "        [-0.0012, -0.0076, -0.0048,  ...,  0.0091, -0.0004,  0.0088],\n",
      "        [-0.0040,  0.0074,  0.0059,  ...,  0.0025,  0.0009, -0.0042]],\n",
      "       requires_grad=True)), ('bias_ih_l0', Parameter containing:\n",
      "tensor([-0.0060,  0.0089, -0.0043,  ...,  0.0025,  0.0033, -0.0072],\n",
      "       requires_grad=True)), ('bias_hh_l0', Parameter containing:\n",
      "tensor([ 0.0023, -0.0029, -0.0025,  ..., -0.0016,  0.0049,  0.0007],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# let's look at what makes up the recurrent layer\n",
    "print(list(model.rnn.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**weight_ih_l0** - weights from the input layer to the recurrent layer\n",
    "\n",
    "**weight_hh_l0** - weights between nodes in the recurrent layer, by default each node is connected to all other nodes!\n",
    "\n",
    "**bias_ih_l0, bias_hh_l0** - the bias modifies the activation of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, the strength of the connection between each pair of nodes is specified\n",
    "model.rnn.weight_hh_l0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0070,  0.0043, -0.0034,  ...,  0.0003, -0.0029,  0.0076],\n",
       "        [ 0.0007, -0.0073, -0.0014,  ...,  0.0099, -0.0019, -0.0050],\n",
       "        [ 0.0044,  0.0034,  0.0088,  ..., -0.0020,  0.0070, -0.0095],\n",
       "        ...,\n",
       "        [ 0.0080, -0.0074, -0.0059,  ...,  0.0088, -0.0020,  0.0023],\n",
       "        [-0.0012, -0.0076, -0.0048,  ...,  0.0091, -0.0004,  0.0088],\n",
       "        [-0.0040,  0.0074,  0.0059,  ...,  0.0025,  0.0009, -0.0042]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these weights are randomly initialised by default - this is what we want to change to add our connectivity constraints\n",
    "model.rnn.weight_hh_l0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate sparse connection matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's make the connections in the recurrent layer randomly sparse. First, we need to decide which connections should remain. To do this, we generate a connectivity matrix, which has the shape (n_nodes x n_nodes) and contains 1s and 0s (1 = the nodes are connected by a trainable weight, 0 = they are not connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_con_matrix(n, p, seed = 42):\n",
    "    np.random.seed(seed) # make things reproducible\n",
    "    con = np.random.rand(n,n) # the connectivity matrix has shape n_nodes x n_nodes\n",
    "    con = np.array(con < p).astype(int) # threshold according to the connection probability\n",
    "    \n",
    "    return torch.tensor(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p is the connection probability (how likely is it that any two nodes are connected?) - here it is 10%\n",
    "a = generate_con_matrix(10, 0.1) \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum() # we would expect about 10% of the weights to be nonzero (10% of 100 = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also generate a fully connected connectivity matrix using this method - that's good for controls\n",
    "b = generate_con_matrix(10, 1.0) \n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "But how do we apply this to our RNN? We just saw that by default, the recurrent layer is fully connected. Enter: torch prune. This lets us remove specified weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(28, 10000, batch_first=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly initialise weights from a uniform distribution\n",
    "model.rnn.weight_hh_l0 = torch.nn.init.uniform_(model.rnn.weight_hh_l0, a=-0.001, b=0.001)\n",
    "\n",
    "# use the torch prune method to remove the weights which have a zero in our connectivity matrix - they won't be trainable\n",
    "# you only need to do this once, when initialising the network. \n",
    "con = generate_con_matrix(10000, 0.1)\n",
    "prune.custom_from_mask(model.rnn, 'weight_hh_l0', con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Under the hood, torch prune inserts what's called a forward_hook. This is a function that is run every time before the model performs a forward pass. This means that all weights will actually get changed when weights are updated by the optimiser, but then prune re-zeroes our sparse weights before the network does anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, <torch.nn.utils.prune.CustomFromMask at 0x2ba740f72af0>)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rnn._forward_pre_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_hh_l0_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]]))]\n"
     ]
    }
   ],
   "source": [
    "# our connectivity matrix is stored here by the network, to be applied at every forward pass\n",
    "print(list(model.rnn.named_buffers())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 1, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con # should be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# basic datasets like MNIST are built into pytorch, so we can just import them:\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/gpfs/soma_fs/scratch/rfruengel/data/MNIST' # set this to wherever you want to keep the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "RNNs accept input as a timeseries - in order to do image recognition with them, we have to split the images up. We present one row of the image at each timestep, so for MNIST, that's 28 timesteps, each with 28 input pixels. For MNIST, which only has one colour channel, this works without further adjustment. The images are 28x28: the RNN will interpret this as 28 timesteps, each with 28 inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set up torch dataloaders to handle batching the data for training and testing\n",
    "train_data = datasets.MNIST(root = data_dir, train = True, transform = ToTensor())\n",
    "test_data = datasets.MNIST(root = data_dir, train = False, transform = ToTensor())\n",
    "\n",
    "loaders = {\n",
    "'train': DataLoader(train_data, batch_size = 100, shuffle = True, num_workers = 1),\n",
    "'test': DataLoader(test_data, batch_size = 100, shuffle = False, num_workers = 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# here, we use cross entropy loss, because our outputs are categorical (classes)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we use the Adam optimiser\n",
    "optimiser = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, model, loaders):\n",
    "    print('------------------------------')\n",
    "    print('num_epochs: {}'.format(num_epochs))\n",
    "    print('model: {}'.format(model))\n",
    "    print('loaders: {}'.format(loaders))\n",
    "    print('training on {}'.format(device))\n",
    "    \n",
    "    total_step = len(loaders['train'])\n",
    "    \n",
    "    print('------------------------------')\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Started epoch: {}'.format(epoch+1))\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, 28, 28).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # pass output to loss function\n",
    "            loss = loss_func(outputs, labels)\n",
    "            \n",
    "            # clear gradients from previous epoch\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # update weights\n",
    "            optimiser.step()\n",
    "            \n",
    "            # print progress\n",
    "            if (i+1)%100 == 0:\n",
    "                print('Step [{} / {}], Loss: {}'.format(i+1, total_step, loss.item()))\n",
    "        \n",
    "        # at the end of each epoch:\n",
    "        model.eval() # put the model in evaluation mode\n",
    "        running_test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): # get test loss/accuracy over whole test dataset\n",
    "            for t, (images, labels) in enumerate(loaders['test']):\n",
    "                images = images.reshape(-1, 28, 28).to(device)\n",
    "                labels = labels.to(device)\n",
    "                # get model predictions\n",
    "                outputs = model(images)\n",
    "                # get the label with the strongest activation, this is the model's class prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # compute test loss\n",
    "                loss = loss_func(outputs, labels)\n",
    "                running_test_loss += loss.item()\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_loss = running_test_loss / t+1\n",
    "        accuracy = 100 * correct/float(total)\n",
    "        model.train() # put it back in training mode\n",
    "        print('TESTING...')\n",
    "        print('Loss: {}, accuracy: {}%'.format(test_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(28, 10000, batch_first=True)\n",
       "  (fc): Linear(in_features=10000, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# everything needed for training must be sent to the GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "num_epochs: 10\n",
      "model: RNN(\n",
      "  (rnn): RNN(28, 10000, batch_first=True)\n",
      "  (fc): Linear(in_features=10000, out_features=10, bias=True)\n",
      ")\n",
      "loaders: {'train': <torch.utils.data.dataloader.DataLoader object at 0x2b148fd0cd90>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x2b14d6e0d730>}\n",
      "training on cuda\n",
      "------------------------------\n",
      "Started epoch: 1\n",
      "Step [100 / 600], Loss: 1.2321842908859253\n",
      "Step [200 / 600], Loss: 0.800067663192749\n",
      "Step [300 / 600], Loss: 0.3912740647792816\n",
      "Step [400 / 600], Loss: 0.5160369873046875\n",
      "Step [500 / 600], Loss: 0.24377603828907013\n",
      "Step [600 / 600], Loss: 0.2154218703508377\n",
      "TESTING...\n",
      "Loss: 1.2641936365627882, accuracy: 92.28%\n",
      "Started epoch: 2\n",
      "Step [100 / 600], Loss: 0.11632823944091797\n",
      "Step [200 / 600], Loss: 0.31488898396492004\n",
      "Step [300 / 600], Loss: 0.39791491627693176\n",
      "Step [400 / 600], Loss: 0.3656487762928009\n",
      "Step [500 / 600], Loss: 0.11766573041677475\n",
      "Step [600 / 600], Loss: 0.10287356376647949\n",
      "TESTING...\n",
      "Loss: 1.1873650754463267, accuracy: 95.01%\n",
      "Started epoch: 3\n",
      "Step [100 / 600], Loss: 0.19203729927539825\n",
      "Step [200 / 600], Loss: 0.11806340515613556\n",
      "Step [300 / 600], Loss: 0.29954496026039124\n",
      "Step [400 / 600], Loss: 0.21643507480621338\n",
      "Step [500 / 600], Loss: 0.03589020296931267\n",
      "Step [600 / 600], Loss: 0.12107232213020325\n",
      "TESTING...\n",
      "Loss: 1.1187375874183318, accuracy: 96.62%\n",
      "Started epoch: 4\n",
      "Step [100 / 600], Loss: 0.1649526208639145\n",
      "Step [200 / 600], Loss: 0.11591091752052307\n",
      "Step [300 / 600], Loss: 0.12721818685531616\n",
      "Step [400 / 600], Loss: 0.2904610335826874\n",
      "Step [500 / 600], Loss: 0.18151213228702545\n",
      "Step [600 / 600], Loss: 0.04736167564988136\n",
      "TESTING...\n",
      "Loss: 1.1226214630261175, accuracy: 96.77%\n",
      "Started epoch: 5\n",
      "Step [100 / 600], Loss: 0.09004385769367218\n",
      "Step [200 / 600], Loss: 0.1454865038394928\n",
      "Step [300 / 600], Loss: 0.03269204497337341\n",
      "Step [400 / 600], Loss: 0.07829709351062775\n",
      "Step [500 / 600], Loss: 0.135447695851326\n",
      "Step [600 / 600], Loss: 0.05208811163902283\n",
      "TESTING...\n",
      "Loss: 1.0976565521747823, accuracy: 97.1%\n",
      "Started epoch: 6\n",
      "Step [100 / 600], Loss: 0.03241325542330742\n",
      "Step [200 / 600], Loss: 0.17109732329845428\n",
      "Step [300 / 600], Loss: 0.028558919206261635\n",
      "Step [400 / 600], Loss: 0.1682354360818863\n",
      "Step [500 / 600], Loss: 0.1590234786272049\n",
      "Step [600 / 600], Loss: 0.09615345299243927\n",
      "TESTING...\n",
      "Loss: 1.0692694561339613, accuracy: 97.98%\n",
      "Started epoch: 7\n",
      "Step [100 / 600], Loss: 0.07980175316333771\n",
      "Step [200 / 600], Loss: 0.06854009628295898\n",
      "Step [300 / 600], Loss: 0.01961580105125904\n",
      "Step [400 / 600], Loss: 0.031018169596791267\n",
      "Step [500 / 600], Loss: 0.05343828350305557\n",
      "Step [600 / 600], Loss: 0.09020764380693436\n",
      "TESTING...\n",
      "Loss: 1.076108828334125, accuracy: 98.02%\n",
      "Started epoch: 8\n",
      "Step [100 / 600], Loss: 0.15065361559391022\n",
      "Step [200 / 600], Loss: 0.030442001298069954\n",
      "Step [300 / 600], Loss: 0.058391720056533813\n",
      "Step [400 / 600], Loss: 0.021330196410417557\n",
      "Step [500 / 600], Loss: 0.009176399558782578\n",
      "Step [600 / 600], Loss: 0.02234208583831787\n",
      "TESTING...\n",
      "Loss: 1.0814553115792065, accuracy: 97.82%\n",
      "Started epoch: 9\n",
      "Step [100 / 600], Loss: 0.02676589973270893\n",
      "Step [200 / 600], Loss: 0.03434811159968376\n",
      "Step [300 / 600], Loss: 0.014236065559089184\n",
      "Step [400 / 600], Loss: 0.015882914885878563\n",
      "Step [500 / 600], Loss: 0.0570329986512661\n",
      "Step [600 / 600], Loss: 0.02824896015226841\n",
      "TESTING...\n",
      "Loss: 1.0655451139230563, accuracy: 98.2%\n",
      "Started epoch: 10\n",
      "Step [100 / 600], Loss: 0.012184806168079376\n",
      "Step [200 / 600], Loss: 0.013687483966350555\n",
      "Step [300 / 600], Loss: 0.027932893484830856\n",
      "Step [400 / 600], Loss: 0.08412136882543564\n",
      "Step [500 / 600], Loss: 0.04707540199160576\n",
      "Step [600 / 600], Loss: 0.0300721675157547\n",
      "TESTING...\n",
      "Loss: 1.053947267572941, accuracy: 98.51%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train(num_epochs, model, loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Parallelised training on MNIST, CIFAR10 or Sleep-EDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Code for Fig. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# make a folder so we can record stats about our different networks while they train\n",
    "outdir = '/path/to/output/folder'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below are delayed dask functions for training sparse RNNs on MNIST, CIFAR10 or Sleep-EDF. They also allow you to record the weight matrix at each epoch, and to record the activations from the hidden layer to the output layer on the test set (warning: this takes up a lot of disk space!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask # we used dask for parallel computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_model_and_train(n_cells, con_p, num_epochs, trackfile_format = '{}_{}', outdir = None, \n",
    "                         torch_seed = 0, early_stop = True, patience = 20,\n",
    "                         weight_track = None, activ_track = None):\n",
    "    '''\n",
    "    Makes a (sparse) RNN, trains it on MNIST and records loss and accuracy and, if set, hidden layer weights and activations.\n",
    "    \n",
    "    n_cells: int, number of nodes in recurrent hidden layer.\n",
    "    con_p: float between 0 and 1, connection probability between any two nodes in the hidden layer, used for generating connectivity matrix.\n",
    "    num_epochs: int, number of epochs to train the model for\n",
    "    trackfile_format: str, base for filename. Will be formatted with .format(n_cells, con_p)\n",
    "    outdir: str, folder path\n",
    "    torch_seed: int, seeds torch and numpy for reproducibility\n",
    "    early_stop: bool, enables early stopping if loss stops decreasing.\n",
    "    patience: int, number of epochs without loss improvement before training is stopped. Only relevant if early_stop = True.\n",
    "    weight_track: None or str, folder path. If None, weights will not be saved.\n",
    "    activ_track: None or str, folder path. If None, activations will not be saved.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def record_test_stats(running_train_loss, i, epoch):\n",
    "        model.eval()\n",
    "        train_loss = running_train_loss / i+1\n",
    "        model.eval()\n",
    "        running_test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): # get test loss over whole test dataset\n",
    "            for t, (images, labels) in enumerate(loaders['test']):\n",
    "                images = images.reshape(-1, 28, 28).to(device)\n",
    "                labels = labels.to(device)\n",
    "                # get model predictions\n",
    "                outputs = model(images, i=t, epoch=epoch)\n",
    "                # get the label with the strongest activation\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # compute test loss\n",
    "                loss = loss_func(outputs, labels)\n",
    "                running_test_loss += loss.item()\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_loss = running_test_loss / t+1\n",
    "        # dump it somewhere\n",
    "        with open(trackfile, 'a') as f:\n",
    "            f.write('{}_{}_{}_{}\\n'.format(epoch, train_loss, test_loss, 100 * correct/float(total)))\n",
    "        model.train()\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    torch.manual_seed(torch_seed)\n",
    "    train_data = datasets.MNIST(root = data_dir, train = True, transform = ToTensor(), download = False)\n",
    "    test_data = datasets.MNIST(root = data_dir, train = False, transform = ToTensor())\n",
    "\n",
    "    loaders = {\n",
    "    'train': DataLoader(train_data, batch_size = 100, shuffle = True, num_workers = 0),\n",
    "    'test': DataLoader(test_data, batch_size = 100, shuffle = True, num_workers = 0)}\n",
    "\n",
    "    ## make the model\n",
    "    class RNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "            super(RNN, self).__init__() # initialise the nn.Module\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, nonlinearity = 'relu', batch_first = True)\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "            self.con_p = con_p # just for writing the track file\n",
    "            self.activ_track = activ_track\n",
    "\n",
    "\n",
    "        def forward(self, x, i=None, epoch=None):\n",
    "            # set initial hidden state\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "            # forward run model\n",
    "            output, hidden = self.rnn(x, h0) # output shape (batch_size, seq_length, hidden_size)\n",
    "            # reshape the output so it fits into the fully connected layer (get the last output from the RNN)\n",
    "            output = output[:, -1, :]\n",
    "            # pass it to the linear layer\n",
    "            output = self.fc(output)\n",
    "            # if tracking activations:\n",
    "            if self.activ_track is not None and i is not None: \n",
    "                with open(self.activ_track.join(trackfile_format.format(self.hidden_size, self.con_p) + '_activations_epoch_{}_{}'.format(epoch, i)), 'wb') as f:\n",
    "                    np.savez(f, recurrent_layer = output.cpu().detach().numpy(), fc_layer = fc_output.cpu().detach().numpy())\n",
    "\n",
    "            return output\n",
    "\n",
    "    model = RNN(28, n_cells, 1, 10)\n",
    "\n",
    "    con = generate_con_matrix(n_cells, con_p, seed = torch_seed)\n",
    "    rnn_layer = model.rnn\n",
    "    rnn_layer.weight_hh_l0 = torch.nn.init.uniform_(rnn_layer.weight_hh_l0, a=-0.001, b=0.001) # randomly initialise weights\n",
    "    prune.custom_from_mask(rnn_layer, 'weight_hh_l0', con) # remove connections not present in the anatomical model\n",
    "\n",
    "    model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimiser = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "    trackfile = outdir.join(trackfile_format.format(n_cells, con_p))\n",
    "\n",
    "    # record test loss before starting\n",
    "    test_loss = record_test_stats(np.nan, np.nan, 'before') \n",
    "\n",
    "    if early_stop:\n",
    "        test_loss_history = test_loss\n",
    "        early_stop_counter = 0\n",
    "    early_stop_flag = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Started epoch: {}'.format(epoch))\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "\n",
    "            images = images.reshape(-1, 28, 28).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # pass output to loss function\n",
    "            loss = loss_func(outputs, labels)\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # update weights\n",
    "            optimiser.step()\n",
    "            # clear gradients from previous epoch\n",
    "            optimiser.zero_grad(set_to_none = True)\n",
    "\n",
    "            # print progress\n",
    "\n",
    "            if i+1 == len(loaders['train']): # at the end of each epoch record stats\n",
    "                if weight_track is not None: # save weights to file\n",
    "                    weights = model.rnn.weight_hh_l0.cpu().detach().numpy()\n",
    "                    weights_fc = model.fc.weight.cpu().detach().numpy()\n",
    "                    assert weights is not None\n",
    "                    with open(weight_track.join(trackfile_format.format(n_cells, con_p) + '_weights_epoch_{}'.format(epoch)), 'wb') as f:\n",
    "                        np.savez(f, recurrent_layer = weights, fc_layer = weights_fc) \n",
    "                test_loss = record_test_stats(running_train_loss, i, epoch)\n",
    "                if early_stop:\n",
    "                    if test_loss < test_loss_history: # if improved\n",
    "                        test_loss_history = test_loss\n",
    "                        early_stop_counter = 0\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "                        if early_stop_counter >= patience:\n",
    "                            early_stop_flag = True\n",
    "        if early_stop_flag:\n",
    "            break\n",
    "\n",
    "    with open(trackfile, 'a') as f:\n",
    "        f.write('training completed successfully!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_model_and_train(n_cells, con_p, num_epochs, trackfile_format = '{}_{}', outdir = None, \n",
    "                         torch_seed = 0, early_stop = True, patience = 20,\n",
    "                         weight_track = None, activ_track = None):\n",
    "    '''\n",
    "    Makes a (sparse) RNN, trains it on CIFAR10 and records loss and accuracy and, if set, hidden layer weights and activations.\n",
    "    \n",
    "    n_cells: int, number of nodes in recurrent hidden layer.\n",
    "    con_p: float between 0 and 1, connection probability between any two nodes in the hidden layer, used for generating connectivity matrix.\n",
    "    num_epochs: int, number of epochs to train the model for\n",
    "    trackfile_format: str, base for filename. Will be formatted with .format(n_cells, con_p)\n",
    "    outdir: str, folder path\n",
    "    torch_seed: int, seeds torch and numpy for reproducibility\n",
    "    early_stop: bool, enables early stopping if loss stops decreasing.\n",
    "    patience: int, number of epochs without loss improvement before training is stopped. Only relevant if early_stop = True.\n",
    "    weight_track: None or str, folder path. If None, weights will not be saved.\n",
    "    activ_track: None or str, folder path. If None, activations will not be saved.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def record_test_stats(running_train_loss, i, epoch):\n",
    "        model.eval()\n",
    "        train_loss = running_train_loss / i+1\n",
    "        model.eval()\n",
    "        running_test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): # get test loss over whole test dataset\n",
    "            for t, (images, labels) in enumerate(loaders['test']):\n",
    "                images = torch.tensor(np.concatenate((images[:,0], images[:,1], images[:,2]), axis = 2)).to(device)\n",
    "                labels = labels.to(device)\n",
    "                # get model predictions\n",
    "                outputs = model(images, i=t, epoch=epoch)\n",
    "                # get the label with the strongest activation\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # compute test loss\n",
    "                loss = loss_func(outputs, labels)\n",
    "                running_test_loss += loss.item()\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_loss = running_test_loss / t+1\n",
    "        # dump it somewhere\n",
    "        with open(trackfile, 'a') as f:\n",
    "            f.write('{}_{}_{}_{}\\n'.format(epoch, train_loss, test_loss, 100 * correct/float(total)))\n",
    "        model.train()\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    torch.manual_seed(torch_seed)\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    train_data = datasets.CIFAR10(root = data_dir, train = True, transform = transform, download = False)\n",
    "    test_data = datasets.CIFAR10(root = data_dir, train = False, transform = transform)\n",
    "\n",
    "    loaders = {\n",
    "    'train': DataLoader(train_data, batch_size = 100, shuffle = True, num_workers = 0),\n",
    "    'test': DataLoader(test_data, batch_size = 100, shuffle = True, num_workers = 0)}\n",
    "\n",
    "    ## make the model\n",
    "    class RNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "            super(RNN, self).__init__() # initialise the nn.Module\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, nonlinearity = 'relu', batch_first = True)\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "            self.con_p = con_p # just for writing the track file\n",
    "            self.activ_track = activ_track\n",
    "\n",
    "\n",
    "        def forward(self, x, i=None, epoch=None):\n",
    "            # set initial hidden state\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "            # forward run model\n",
    "            output, hidden = self.rnn(x, h0) # output shape (batch_size, seq_length, hidden_size)\n",
    "            # reshape the output so it fits into the fully connected layer (get the last output from the RNN)\n",
    "            output = output[:, -1, :]\n",
    "            # pass it to the linear layer\n",
    "            output = self.fc(output)\n",
    "            # if tracking activations:\n",
    "            if self.activ_track is not None and i is not None: \n",
    "                with open(self.activ_track.join(trackfile_format.format(self.hidden_size, self.con_p) + '_activations_epoch_{}_{}'.format(epoch, i)), 'wb') as f:\n",
    "                    np.savez(f, recurrent_layer = output.cpu().detach().numpy(), fc_layer = fc_output.cpu().detach().numpy())\n",
    "\n",
    "            return output\n",
    "\n",
    "    model = RNN(96, n_cells, 1, 10)\n",
    "\n",
    "    con = generate_con_matrix(n_cells, con_p, seed = torch_seed)\n",
    "    rnn_layer = model.rnn\n",
    "    rnn_layer.weight_hh_l0 = torch.nn.init.uniform_(rnn_layer.weight_hh_l0, a=-0.001, b=0.001) # randomly initialise weights\n",
    "    prune.custom_from_mask(rnn_layer, 'weight_hh_l0', con) # remove connections not present in the anatomical model\n",
    "\n",
    "    model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimiser = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "    trackfile = outdir.join(trackfile_format.format(n_cells, con_p))\n",
    "\n",
    "    # record test loss before starting\n",
    "    test_loss = record_test_stats(np.nan, np.nan, 'before') \n",
    "\n",
    "    if early_stop:\n",
    "        test_loss_history = test_loss\n",
    "        early_stop_counter = 0\n",
    "    early_stop_flag = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Started epoch: {}'.format(epoch))\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "\n",
    "            images = torch.tensor(np.concatenate((images[:,0], images[:,1], images[:,2]), axis = 2)).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # pass output to loss function\n",
    "            loss = loss_func(outputs, labels)\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # update weights\n",
    "            optimiser.step()\n",
    "            # clear gradients from previous epoch\n",
    "            optimiser.zero_grad(set_to_none = True)\n",
    "\n",
    "            # print progress\n",
    "\n",
    "            if i+1 == len(loaders['train']): # at the end of each epoch record stats\n",
    "                if weight_track is not None: # save weights to file\n",
    "                    weights = model.rnn.weight_hh_l0.cpu().detach().numpy()\n",
    "                    weights_fc = model.fc.weight.cpu().detach().numpy()\n",
    "                    assert weights is not None\n",
    "                    with open(weight_track.join(trackfile_format.format(n_cells, con_p) + '_weights_epoch_{}'.format(epoch)), 'wb') as f:\n",
    "                        np.savez(f, recurrent_layer = weights, fc_layer = weights_fc) \n",
    "                test_loss = record_test_stats(running_train_loss, i, epoch)\n",
    "                if early_stop:\n",
    "                    if test_loss < test_loss_history: # if improved\n",
    "                        test_loss_history = test_loss\n",
    "                        early_stop_counter = 0\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "                        if early_stop_counter >= patience:\n",
    "                            early_stop_flag = True\n",
    "        if early_stop_flag:\n",
    "            break\n",
    "\n",
    "    with open(trackfile, 'a') as f:\n",
    "        f.write('training completed successfully!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import torchtime which bundles the Sleep-EDF dataset in a preprocessed format\n",
    "import torchtime\n",
    "from torchtime.data import UEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_model_and_train(n_cells, con_p, num_epochs, trackfile_format = '{}_{}', outdir = None, \n",
    "                         torch_seed = 0, early_stop = True, patience = 20,\n",
    "                         weight_track = None, activ_track = None):\n",
    "    '''\n",
    "    Makes a (sparse) RNN, trains it on Sleep-EDF and records loss and accuracy and, if set, hidden layer weights and activations.\n",
    "    \n",
    "    n_cells: int, number of nodes in recurrent hidden layer.\n",
    "    con_p: float between 0 and 1, connection probability between any two nodes in the hidden layer, used for generating connectivity matrix.\n",
    "    num_epochs: int, number of epochs to train the model for\n",
    "    trackfile_format: str, base for filename. Will be formatted with .format(n_cells, con_p)\n",
    "    outdir: str, folder path\n",
    "    torch_seed: int, seeds torch and numpy for reproducibility\n",
    "    early_stop: bool, enables early stopping if loss stops decreasing.\n",
    "    patience: int, number of epochs without loss improvement before training is stopped. Only relevant if early_stop = True.\n",
    "    weight_track: None or str, folder path. If None, weights will not be saved.\n",
    "    activ_track: None or str, folder path. If None, activations will not be saved.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def record_test_stats(running_train_loss, i, epoch):\n",
    "        model.eval()\n",
    "        train_loss = running_train_loss / i+1\n",
    "        model.eval()\n",
    "        running_test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): # get test loss over whole test dataset\n",
    "            for t, (images, labels) in enumerate(loaders['test']):\n",
    "                images = dat['X'][:, :, 1].unsqueeze(axis = 2).to(device)\n",
    "                labels = dat['y'].to(device)\n",
    "                # get model predictions\n",
    "                outputs = model(images, i=t, epoch=epoch)\n",
    "                # get the label with the strongest activation\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # compute test loss\n",
    "                loss = loss_func(outputs, labels)\n",
    "                running_test_loss += loss.item()\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_loss = running_test_loss / t+1\n",
    "        # dump it somewhere\n",
    "        with open(trackfile, 'a') as f:\n",
    "            f.write('{}_{}_{}_{}\\n'.format(epoch, train_loss, test_loss, 100 * correct/float(total)))\n",
    "        model.train()\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    torch.manual_seed(torch_seed)\n",
    "    test_data = UEA(\n",
    "        dataset=\"Sleep\",\n",
    "        split=\"val\",\n",
    "        train_prop=0.7,\n",
    "        seed=123,  # for reproducibility\n",
    "        path = '/gpfs/soma_fs/scratch/rfruengel/data/sleep_EEG_torchtime'\n",
    "    )\n",
    "    train_data = UEA(\n",
    "        dataset=\"Sleep\",\n",
    "        split=\"train\",\n",
    "        train_prop=0.7,\n",
    "        seed=123,  # for reproducibility\n",
    "        path = '/gpfs/soma_fs/scratch/rfruengel/data/sleep_EEG_torchtime'\n",
    "    )\n",
    "\n",
    "    loaders = {\n",
    "    'train': DataLoader(train_data, batch_size = 100, shuffle = True, num_workers = 0),\n",
    "    'test': DataLoader(test_data, batch_size = 100, shuffle = True, num_workers = 0)}\n",
    "\n",
    "    ## make the model\n",
    "    class RNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "            super(RNN, self).__init__() # initialise the nn.Module\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, nonlinearity = 'relu', batch_first = True)\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "            self.con_p = con_p # just for writing the track file\n",
    "            self.activ_track = activ_track\n",
    "\n",
    "\n",
    "        def forward(self, x, i=None, epoch=None):\n",
    "            # set initial hidden state\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "            # forward run model\n",
    "            output, hidden = self.rnn(x, h0) # output shape (batch_size, seq_length, hidden_size)\n",
    "            # reshape the output so it fits into the fully connected layer (get the last output from the RNN)\n",
    "            output = output[:, -1, :]\n",
    "            # pass it to the linear layer\n",
    "            output = self.fc(output)\n",
    "            # if tracking activations:\n",
    "            if self.activ_track is not None and i is not None: \n",
    "                with open(self.activ_track.join(trackfile_format.format(self.hidden_size, self.con_p) + '_activations_epoch_{}_{}'.format(epoch, i)), 'wb') as f:\n",
    "                    np.savez(f, recurrent_layer = output.cpu().detach().numpy(), fc_layer = fc_output.cpu().detach().numpy())\n",
    "\n",
    "            return output\n",
    "\n",
    "    model = RNN(1, n_cells, 1, 5)\n",
    "\n",
    "    con = generate_con_matrix(n_cells, con_p, seed = torch_seed)\n",
    "    rnn_layer = model.rnn\n",
    "    rnn_layer.weight_hh_l0 = torch.nn.init.uniform_(rnn_layer.weight_hh_l0, a=-0.001, b=0.001) # randomly initialise weights\n",
    "    prune.custom_from_mask(rnn_layer, 'weight_hh_l0', con) # remove connections not present in the anatomical model\n",
    "\n",
    "    model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimiser = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "    trackfile = outdir.join(trackfile_format.format(n_cells, con_p))\n",
    "\n",
    "    # record test loss before starting\n",
    "    test_loss = record_test_stats(np.nan, np.nan, 'before') \n",
    "\n",
    "    if early_stop:\n",
    "        test_loss_history = test_loss\n",
    "        early_stop_counter = 0\n",
    "    early_stop_flag = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Started epoch: {}'.format(epoch))\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "\n",
    "            images = dat['X'][:, :, 1].unsqueeze(axis = 2).to(device)\n",
    "            labels = dat['y'].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # pass output to loss function\n",
    "            loss = loss_func(outputs, labels)\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # update weights\n",
    "            optimiser.step()\n",
    "            # clear gradients from previous epoch\n",
    "            optimiser.zero_grad(set_to_none = True)\n",
    "\n",
    "            # print progress\n",
    "\n",
    "            if i+1 == len(loaders['train']): # at the end of each epoch record stats\n",
    "                if weight_track is not None: # save weights to file\n",
    "                    weights = model.rnn.weight_hh_l0.cpu().detach().numpy()\n",
    "                    weights_fc = model.fc.weight.cpu().detach().numpy()\n",
    "                    assert weights is not None\n",
    "                    with open(weight_track.join(trackfile_format.format(n_cells, con_p) + '_weights_epoch_{}'.format(epoch)), 'wb') as f:\n",
    "                        np.savez(f, recurrent_layer = weights, fc_layer = weights_fc) \n",
    "                test_loss = record_test_stats(running_train_loss, i, epoch)\n",
    "                if early_stop:\n",
    "                    if test_loss < test_loss_history: # if improved\n",
    "                        test_loss_history = test_loss\n",
    "                        early_stop_counter = 0\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "                        if early_stop_counter >= patience:\n",
    "                            early_stop_flag = True\n",
    "        if early_stop_flag:\n",
    "            break\n",
    "\n",
    "    with open(trackfile, 'a') as f:\n",
    "        f.write('training completed successfully!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Equivalent code for training feedforward networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_model_and_train(hidden_size, con_p, num_layers, num_epochs, trackfile_format = '{}_{}', \n",
    "                         outdir = None, torch_seed = 0, early_stop = True, patience = 20):\n",
    "    '''\n",
    "    Makes a (sparse) feedforward ANN, trains it on MNIST and records loss and accuracy\n",
    "    \n",
    "    hidden_size: int, number of nodes in each hidden layer.\n",
    "    con_p: float between 0 and 1, connection probability from a hidden node in one layer to the next layer\n",
    "    num_layers: int, number of hidden layers (2 was used in the paper)\n",
    "    num_epochs: int, number of epochs to train the model for\n",
    "    trackfile_format: str, base for filename. Will be formatted with .format(n_cells, con_p)\n",
    "    outdir: str, folder path\n",
    "    torch_seed: int, seeds torch and numpy for reproducibility\n",
    "    early_stop: bool, enables early stopping if loss stops decreasing.\n",
    "    patience: int, number of epochs without loss improvement before training is stopped. Only relevant if early_stop = True.\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def record_test_stats(running_train_loss, i, epoch):\n",
    "        model.eval()\n",
    "        train_loss = running_train_loss / i+1\n",
    "        model.eval()\n",
    "        running_test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): # get test loss over whole test dataset\n",
    "            for t, (images, labels) in enumerate(loaders['test']):\n",
    "                images = images.reshape(-1, 28*28).to(device)\n",
    "                labels = labels.to(device)\n",
    "                # get model predictions\n",
    "                outputs = model(images)\n",
    "                # get the label with the strongest activation\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # compute test loss\n",
    "                loss = loss_func(outputs, labels)\n",
    "                running_test_loss += loss.item()\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_loss = running_test_loss / t+1\n",
    "        # dump it somewhere\n",
    "        with open(trackfile, 'a') as f:\n",
    "            f.write('{}_{}_{}_{}\\n'.format(epoch, train_loss, test_loss, 100 * correct/float(total)))\n",
    "        model.train()\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    torch.manual_seed(torch_seed)\n",
    "    train_data = datasets.MNIST(root = data_dir, train = True, transform = ToTensor(), download = False)\n",
    "    test_data = datasets.MNIST(root = data_dir, train = False, transform = ToTensor())\n",
    "\n",
    "    loaders = {\n",
    "    'train': DataLoader(train_data, batch_size = 100, shuffle = True, num_workers = 0),\n",
    "    'test': DataLoader(test_data, batch_size = 100, shuffle = True, num_workers = 0)}\n",
    "\n",
    "    ## make the model\n",
    "    class feedforward(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "            super(feedforward, self).__init__() # initialise the nn.Module\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "\n",
    "            self.l1 = nn.Linear(input_size, hidden_size)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.linears = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for i in range(num_layers-1)])\n",
    "\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            # forward run model\n",
    "            out = self.l1(x)\n",
    "            out = self.relu(out)\n",
    "\n",
    "            for lin in self.linears:\n",
    "                out = lin(out)\n",
    "                out = self.relu(out)\n",
    "\n",
    "            # pass it to the linear layer\n",
    "            output = self.fc(out)\n",
    "\n",
    "            return output\n",
    "\n",
    "    model = feedforward(28*28, hidden_size, num_layers, 10)\n",
    "\n",
    "    for l in range(num_layers-1):\n",
    "        con = generate_con_matrix(hidden_size, con_p)\n",
    "        lin_layer = model.linears[l]\n",
    "        lin_layer.weight = torch.nn.init.uniform_(lin_layer.weight, a=-0.001, b=0.001) # randomly initialise weights\n",
    "        prune.custom_from_mask(lin_layer, 'weight', con) # remove connections to obtain sparsity\n",
    "    model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimiser = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "    trackfile = outdir.join(trackfile_format.format(hidden_size, num_layers, con_p))\n",
    "\n",
    "    # record test loss before starting\n",
    "    test_loss = record_test_stats(np.nan, np.nan, 'before') \n",
    "\n",
    "    if early_stop:\n",
    "        test_loss_history = test_loss\n",
    "        early_stop_counter = 0\n",
    "    early_stop_flag = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Started epoch: {}'.format(epoch))\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # pass output to loss function\n",
    "            loss = loss_func(outputs, labels)\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # update weights\n",
    "            optimiser.step()\n",
    "            # clear gradients from previous epoch\n",
    "            optimiser.zero_grad(set_to_none = True)\n",
    "\n",
    "            # print progress\n",
    "\n",
    "            if i+1 == len(loaders['train']): # at the end of each epoch record stats\n",
    "                test_loss = record_test_stats(running_train_loss, i, epoch)\n",
    "                if early_stop:\n",
    "                    if test_loss < test_loss_history: # if improved\n",
    "                        test_loss_history = test_loss\n",
    "                        early_stop_counter = 0\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "                        if early_stop_counter >= patience:\n",
    "                            early_stop_flag = True\n",
    "        if early_stop_flag:\n",
    "            break\n",
    "\n",
    "    with open(trackfile, 'a') as f:\n",
    "        f.write('training completed successfully!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_model_and_train(hidden_size, con_p, num_layers, num_epochs, trackfile_format = '{}_{}', \n",
    "                         outdir = None, torch_seed = 0, early_stop = True, patience = 20):\n",
    "    '''\n",
    "    Makes a (sparse) feedforward ANN, trains it on CIFAR10 and records loss and accuracy\n",
    "    \n",
    "    hidden_size: int, number of nodes in each hidden layer.\n",
    "    con_p: float between 0 and 1, connection probability from a hidden node in one layer to the next layer\n",
    "    num_layers: int, number of hidden layers (2 was used in the paper)\n",
    "    num_epochs: int, number of epochs to train the model for\n",
    "    trackfile_format: str, base for filename. Will be formatted with .format(n_cells, con_p)\n",
    "    outdir: str, folder path\n",
    "    torch_seed: int, seeds torch and numpy for reproducibility\n",
    "    early_stop: bool, enables early stopping if loss stops decreasing.\n",
    "    patience: int, number of epochs without loss improvement before training is stopped. Only relevant if early_stop = True.\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def record_test_stats(running_train_loss, i, epoch):\n",
    "        model.eval()\n",
    "        train_loss = running_train_loss / i+1\n",
    "        model.eval()\n",
    "        running_test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): # get test loss over whole test dataset\n",
    "            for t, (images, labels) in enumerate(loaders['test']):\n",
    "                images = images.reshape(-1, 32*32*3).to(device)\n",
    "                labels = labels.to(device)\n",
    "                # get model predictions\n",
    "                outputs = model(images)\n",
    "                # get the label with the strongest activation\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # compute test loss\n",
    "                loss = loss_func(outputs, labels)\n",
    "                running_test_loss += loss.item()\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_loss = running_test_loss / t+1\n",
    "        # dump it somewhere\n",
    "        with open(trackfile, 'a') as f:\n",
    "            f.write('{}_{}_{}_{}\\n'.format(epoch, train_loss, test_loss, 100 * correct/float(total)))\n",
    "        model.train()\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    torch.manual_seed(torch_seed)\n",
    "    train_data = datasets.CIFAR10(root = data_dir, train = True, transform = transform, download = False)\n",
    "    test_data = datasets.CIFAR10(root = data_dir, train = False, transform = transform)\n",
    "\n",
    "    loaders = {\n",
    "    'train': DataLoader(train_data, batch_size = 100, shuffle = True, num_workers = 0),\n",
    "    'test': DataLoader(test_data, batch_size = 100, shuffle = True, num_workers = 0)}\n",
    "\n",
    "    ## make the model\n",
    "    class feedforward(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "            super(feedforward, self).__init__() # initialise the nn.Module\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "\n",
    "            self.l1 = nn.Linear(input_size, hidden_size)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.linears = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for i in range(num_layers-1)])\n",
    "\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            # forward run model\n",
    "            out = self.l1(x)\n",
    "            out = self.relu(out)\n",
    "\n",
    "            for lin in self.linears:\n",
    "                out = lin(out)\n",
    "                out = self.relu(out)\n",
    "\n",
    "            # pass it to the linear layer\n",
    "            output = self.fc(out)\n",
    "\n",
    "            return output\n",
    "\n",
    "    model = feedforward(32*32*3, hidden_size, num_layers, 10)\n",
    "\n",
    "    for l in range(num_layers-1):\n",
    "        con = generate_con_matrix(hidden_size, con_p)\n",
    "        lin_layer = model.linears[l]\n",
    "        lin_layer.weight = torch.nn.init.uniform_(lin_layer.weight, a=-0.001, b=0.001) # randomly initialise weights\n",
    "        prune.custom_from_mask(lin_layer, 'weight', con) # remove connections to obtain sparsity\n",
    "    model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimiser = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "    trackfile = outdir.join(trackfile_format.format(hidden_size, num_layers, con_p))\n",
    "\n",
    "    # record test loss before starting\n",
    "    test_loss = record_test_stats(np.nan, np.nan, 'before') \n",
    "\n",
    "    if early_stop:\n",
    "        test_loss_history = test_loss\n",
    "        early_stop_counter = 0\n",
    "    early_stop_flag = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Started epoch: {}'.format(epoch))\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "\n",
    "            images = images.reshape(-1, 32*32*3).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # pass output to loss function\n",
    "            loss = loss_func(outputs, labels)\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # update weights\n",
    "            optimiser.step()\n",
    "            # clear gradients from previous epoch\n",
    "            optimiser.zero_grad(set_to_none = True)\n",
    "\n",
    "            # print progress\n",
    "\n",
    "            if i+1 == len(loaders['train']): # at the end of each epoch record stats\n",
    "                test_loss = record_test_stats(running_train_loss, i, epoch)\n",
    "                if early_stop:\n",
    "                    if test_loss < test_loss_history: # if improved\n",
    "                        test_loss_history = test_loss\n",
    "                        early_stop_counter = 0\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "                        if early_stop_counter >= patience:\n",
    "                            early_stop_flag = True\n",
    "        if early_stop_flag:\n",
    "            break\n",
    "\n",
    "    with open(trackfile, 'a') as f:\n",
    "        f.write('training completed successfully!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_model_and_train(hidden_size, con_p, num_layers, num_epochs, trackfile_format = '{}_{}', \n",
    "                         outdir = None, torch_seed = 0, early_stop = True, patience = 20):\n",
    "    '''\n",
    "    Makes a (sparse) feedforward ANN, trains it on Sleep-EDF and records loss and accuracy\n",
    "    \n",
    "    hidden_size: int, number of nodes in each hidden layer.\n",
    "    con_p: float between 0 and 1, connection probability from a hidden node in one layer to the next layer\n",
    "    num_layers: int, number of hidden layers (2 was used in the paper)\n",
    "    num_epochs: int, number of epochs to train the model for\n",
    "    trackfile_format: str, base for filename. Will be formatted with .format(n_cells, con_p)\n",
    "    outdir: str, folder path\n",
    "    torch_seed: int, seeds torch and numpy for reproducibility\n",
    "    early_stop: bool, enables early stopping if loss stops decreasing.\n",
    "    patience: int, number of epochs without loss improvement before training is stopped. Only relevant if early_stop = True.\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def record_test_stats(running_train_loss, i, epoch):\n",
    "        model.eval()\n",
    "        train_loss = running_train_loss / i+1\n",
    "        model.eval()\n",
    "        running_test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): # get test loss over whole test dataset\n",
    "            for t, (images, labels) in enumerate(loaders['test']):\n",
    "                images = dat['X'][:, :, 1].to(device)\n",
    "                labels = dat['y'].to(device)\n",
    "                # get model predictions\n",
    "                outputs = model(images)\n",
    "                # get the label with the strongest activation\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # compute test loss\n",
    "                loss = loss_func(outputs, labels)\n",
    "                running_test_loss += loss.item()\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_loss = running_test_loss / t+1\n",
    "        # dump it somewhere\n",
    "        with open(trackfile, 'a') as f:\n",
    "            f.write('{}_{}_{}_{}\\n'.format(epoch, train_loss, test_loss, 100 * correct/float(total)))\n",
    "        model.train()\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    torch.manual_seed(torch_seed)\n",
    "    test_data = UEA(\n",
    "        dataset=\"Sleep\",\n",
    "        split=\"val\",\n",
    "        train_prop=0.7,\n",
    "        seed=123,  # for reproducibility\n",
    "        path = '/gpfs/soma_fs/scratch/rfruengel/data/sleep_EEG_torchtime'\n",
    "    )\n",
    "    train_data = UEA(\n",
    "        dataset=\"Sleep\",\n",
    "        split=\"train\",\n",
    "        train_prop=0.7,\n",
    "        seed=123,  # for reproducibility\n",
    "        path = '/gpfs/soma_fs/scratch/rfruengel/data/sleep_EEG_torchtime'\n",
    "    )\n",
    "\n",
    "    loaders = {\n",
    "    'train': DataLoader(train_data, batch_size = 100, shuffle = True, num_workers = 0),\n",
    "    'test': DataLoader(test_data, batch_size = 100, shuffle = True, num_workers = 0)}\n",
    "\n",
    "    ## make the model\n",
    "    class feedforward(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "            super(feedforward, self).__init__() # initialise the nn.Module\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "\n",
    "            self.l1 = nn.Linear(input_size, hidden_size)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.linears = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for i in range(num_layers-1)])\n",
    "\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            # forward run model\n",
    "            out = self.l1(x)\n",
    "            out = self.relu(out)\n",
    "\n",
    "            for lin in self.linears:\n",
    "                out = lin(out)\n",
    "                out = self.relu(out)\n",
    "\n",
    "            # pass it to the linear layer\n",
    "            output = self.fc(out)\n",
    "\n",
    "            return output\n",
    "\n",
    "    model = feedforward(178, n_cells, 2, 5)\n",
    "\n",
    "    for l in range(num_layers-1):\n",
    "        con = generate_con_matrix(hidden_size, con_p)\n",
    "        lin_layer = model.linears[l]\n",
    "        lin_layer.weight = torch.nn.init.uniform_(lin_layer.weight, a=-0.001, b=0.001) # randomly initialise weights\n",
    "        prune.custom_from_mask(lin_layer, 'weight', con) # remove connections to obtain sparsity\n",
    "    model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimiser = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "    trackfile = outdir.join(trackfile_format.format(hidden_size, num_layers, con_p))\n",
    "\n",
    "    # record test loss before starting\n",
    "    test_loss = record_test_stats(np.nan, np.nan, 'before') \n",
    "\n",
    "    if early_stop:\n",
    "        test_loss_history = test_loss\n",
    "        early_stop_counter = 0\n",
    "    early_stop_flag = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Started epoch: {}'.format(epoch))\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "\n",
    "            images = dat['X'][:, :, 1].to(device)\n",
    "            labels = dat['y'].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # pass output to loss function\n",
    "            loss = loss_func(outputs, labels)\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # update weights\n",
    "            optimiser.step()\n",
    "            # clear gradients from previous epoch\n",
    "            optimiser.zero_grad(set_to_none = True)\n",
    "\n",
    "            # print progress\n",
    "\n",
    "            if i+1 == len(loaders['train']): # at the end of each epoch record stats\n",
    "                test_loss = record_test_stats(running_train_loss, i, epoch)\n",
    "                if early_stop:\n",
    "                    if test_loss < test_loss_history: # if improved\n",
    "                        test_loss_history = test_loss\n",
    "                        early_stop_counter = 0\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "                        if early_stop_counter >= patience:\n",
    "                            early_stop_flag = True\n",
    "        if early_stop_flag:\n",
    "            break\n",
    "\n",
    "    with open(trackfile, 'a') as f:\n",
    "        f.write('training completed successfully!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Reduced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Code for Fig. 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_training_subset(train_data, samples_per_class = 1000):\n",
    "    c = defaultdict(list)\n",
    "    for i, t in enumerate(train_data.targets.tolist()):\n",
    "        c[t].append(i)\n",
    "    \n",
    "    for t in sorted(list(set(train_data.targets.tolist()))):\n",
    "        examples = c[t]\n",
    "        np.random.shuffle(examples)\n",
    "        c[t] = examples[:samples_per_class]\n",
    "    \n",
    "    indices = [int(i) for x in c.values() for i in x]\n",
    "    \n",
    "    assert len(indices) == samples_per_class * len(list(set(train_data.targets.tolist())))\n",
    "    \n",
    "    tr_reduced = torch.utils.data.Subset(train_data, indices)\n",
    "    return tr_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# applied to the training dataset as follows, all remaining model and training code unchanged\n",
    "train_data = datasets.MNIST(root = data_dir, train = True, transform = ToTensor(), download = False)\n",
    "test_data = datasets.MNIST(root = data_dir, train = False, transform = ToTensor())\n",
    "\n",
    "train_partial = get_training_subset(train_data, samples_per_class = samples_per_class) ## \n",
    "\n",
    "loaders = {\n",
    "'train': DataLoader(train_partial, batch_size = 100, shuffle = True, num_workers = 0), ## \n",
    "'test': DataLoader(test_data, batch_size = 100, shuffle = True, num_workers = 0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Node dropout noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Code for Fig. 5D&E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# modify the forward pass. We set dropout = True only during the test set\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__() # initialise the nn.Module\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, nonlinearity = 'relu', batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x, dropout = False):\n",
    "        # set initial hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # forward run model\n",
    "        output, hidden = self.rnn(x, h0) # output shape (batch_size, seq_length, hidden_size)\n",
    "        # reshape the output so it fits into the fully connected layer (get the last output from the RNN)\n",
    "        output = output[:, -1, :]\n",
    "        if dropout:\n",
    "            random_mask = np.random.rand(100, n_cells) # 100 is the batch size -> a different subset of nodes is dropped for each image\n",
    "            random_mask = np.array(random_mask > dropout_fraction).astype(int)\n",
    "            random_mask = torch.tensor(random_mask)\n",
    "            random_mask = random_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = output*random_mask\n",
    "\n",
    "        # pass it to the linear layer\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dale's principle networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Code for Fig. 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below is our implementation of Dale's networks. First, randomly choose a proportion (11.5% and 50% were reported in the paper) of nodes to be inhibitory when initialising the ANN. Inhibitory nodes should have all negative outgoing weights, while excitatory nodes should have all positive outgoing weights. Then, whenever the weights of these nodes get updated during training, check to see if they changed sign - if they did, set the weight to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# to ensure reproducibility, we should set the torch and numpy seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## make the model - a recurrent network with one hidden layer, and a fully connected output layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__() # initialise the nn.Module\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, nonlinearity = 'relu', batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # set initial hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # pass input throught the recurrent layer\n",
    "        output, hidden = self.rnn(x, h0) # output shape (batch_size, seq_length, hidden_size)\n",
    "        # reshape the output so it fits into the fully connected layer (get the last output from the RNN)\n",
    "        output = output[:, -1, :]\n",
    "        # pass it to the linear layer to get the classification\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_size = 28 # the size of the input at each timestep\n",
    "hidden_size = 10000 # how many nodes do we want in the hidden layer?\n",
    "num_layers = 1 # how many recurrent layers \n",
    "num_classes = 10 # this is defined by the dataset, which has 10 classes (digits 0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "con = generate_con_matrix(hidden_size, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inh_percent = 0.115 # the proportion of inhibitory cells in somatosensory cortex https://doi.org/10.1073/pnas.1113648108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select the nodes which we will make inhibitory\n",
    "inh_nodes = np.random.choice(list(range(hidden_size)), size=int(inh_percent*hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(28, 10000, batch_first=True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly initialise weights from a uniform distribution, this time all positive\n",
    "model.rnn.weight_hh_l0 = torch.nn.init.uniform_(model.rnn.weight_hh_l0, a=0, b=0.001) \n",
    "\n",
    "# now, make all the weights of our inhibitory nodes negative by multiplying them by -1\n",
    "with torch.no_grad():\n",
    "    model.rnn.weight_hh_l0[:, inh_nodes]*=-1 \n",
    "\n",
    "# remove connections to make it sparse\n",
    "prune.custom_from_mask(model.rnn, 'weight_hh_l0', con) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, model, loaders):\n",
    "    print('------------------------------')\n",
    "    print('num_epochs: {}'.format(num_epochs))\n",
    "    print('model: {}'.format(model))\n",
    "    print('loaders: {}'.format(loaders))\n",
    "    print('training on {}'.format(device))\n",
    "    \n",
    "    total_step = len(loaders['train'])\n",
    "    \n",
    "    # NEW: record which weights started as positive\n",
    "    pweights = model.rnn.weight_hh_l0 >= 0\n",
    "    pweights = pweights.to(device)\n",
    "    \n",
    "    print('------------------------------')\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Started epoch: {}'.format(epoch))\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            images = images.reshape(-1, 28, 28).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # pass output to loss function\n",
    "            loss = loss_func(outputs, labels)\n",
    "            \n",
    "            # clear gradients from previous epoch\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # update weights\n",
    "            optimiser.step()\n",
    "            \n",
    "            # NEW: check which weights changed sign, set them to zero                \n",
    "            with torch.no_grad(): # if we want to make manual changes to the weights like setting them to zero, we have to use torch.no_grad\n",
    "                model.rnn.weight_hh_l0[pweights & (~(model.rnn.weight_hh_l0 > 0))] = 0\n",
    "                model.rnn.weight_hh_l0[~pweights & (model.rnn.weight_hh_l0 > 0)] = 0\n",
    "            \n",
    "            # print progress\n",
    "            if (i+1)%100 == 0:\n",
    "                print('Step [{} / {}], Loss: {}'.format(i+1, total_step, loss.item()))\n",
    "        \n",
    "        # at the end of each epoch:\n",
    "        model.eval() # put the model in evaluation mode. This freezes certain things that might be going on during training, e.g. dropout. We don't use it, but it's good practice to include anyway.\n",
    "        running_test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): # get test loss over whole test dataset\n",
    "            for t, (images, labels) in enumerate(loaders['test']):\n",
    "                images = images.reshape(-1, 28, 28).to(device)\n",
    "                labels = labels.to(device)\n",
    "                # get model predictions\n",
    "                outputs = model(images)\n",
    "                # get the label with the strongest activation, we will say this is the model's class prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # compute test loss\n",
    "                loss = loss_func(outputs, labels)\n",
    "                running_test_loss += loss.item()\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_loss = running_test_loss / t+1\n",
    "        accuracy = 100 * correct/float(total)\n",
    "        model.train() # put it back in training mode\n",
    "        print('TESTING...')\n",
    "        print('Loss: {}, accuracy: {}%'.format(test_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(28, 10000, batch_first=True)\n",
       "  (fc): Linear(in_features=10000, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "num_epochs: 10\n",
      "model: RNN(\n",
      "  (rnn): RNN(28, 10000, batch_first=True)\n",
      "  (fc): Linear(in_features=10000, out_features=10, bias=True)\n",
      ")\n",
      "loaders: {'train': <torch.utils.data.dataloader.DataLoader object at 0x2b148fd0cd90>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x2b14d6e0d730>}\n",
      "training on cuda\n",
      "------------------------------\n",
      "Started epoch: 0\n",
      "Step [100 / 600], Loss: 1.4706289768218994\n",
      "Step [200 / 600], Loss: 0.8616660833358765\n",
      "Step [300 / 600], Loss: 0.7347720861434937\n",
      "Step [400 / 600], Loss: 0.24932155013084412\n",
      "Step [500 / 600], Loss: 0.22504915297031403\n",
      "Step [600 / 600], Loss: 0.22148172557353973\n",
      "TESTING...\n",
      "Loss: 1.3064008157587412, accuracy: 91.05%\n",
      "Started epoch: 1\n",
      "Step [100 / 600], Loss: 0.36837124824523926\n",
      "Step [200 / 600], Loss: 0.20446786284446716\n",
      "Step [300 / 600], Loss: 0.17891612648963928\n",
      "Step [400 / 600], Loss: 0.2340857833623886\n",
      "Step [500 / 600], Loss: 0.14661064743995667\n",
      "Step [600 / 600], Loss: 0.24163366854190826\n",
      "TESTING...\n",
      "Loss: 1.1795858990099997, accuracy: 95.02%\n",
      "Started epoch: 2\n",
      "Step [100 / 600], Loss: 0.259349524974823\n",
      "Step [200 / 600], Loss: 0.18733611702919006\n",
      "Step [300 / 600], Loss: 0.17854928970336914\n",
      "Step [400 / 600], Loss: 0.1217857077717781\n",
      "Step [500 / 600], Loss: 0.15485377609729767\n",
      "Step [600 / 600], Loss: 0.16979652643203735\n",
      "TESTING...\n",
      "Loss: 1.173472779092727, accuracy: 95.17%\n",
      "Started epoch: 3\n",
      "Step [100 / 600], Loss: 0.13916146755218506\n",
      "Step [200 / 600], Loss: 0.11364017426967621\n",
      "Step [300 / 600], Loss: 0.20692914724349976\n",
      "Step [400 / 600], Loss: 0.10473182797431946\n",
      "Step [500 / 600], Loss: 0.12722742557525635\n",
      "Step [600 / 600], Loss: 0.06702021509408951\n",
      "TESTING...\n",
      "Loss: 1.0964343856991918, accuracy: 97.18%\n",
      "Started epoch: 4\n",
      "Step [100 / 600], Loss: 0.024510957300662994\n",
      "Step [200 / 600], Loss: 0.07295137643814087\n",
      "Step [300 / 600], Loss: 0.09829099476337433\n",
      "Step [400 / 600], Loss: 0.038957200944423676\n",
      "Step [500 / 600], Loss: 0.15391166508197784\n",
      "Step [600 / 600], Loss: 0.07574830204248428\n",
      "TESTING...\n",
      "Loss: 1.0846900481135688, accuracy: 97.45%\n",
      "Started epoch: 5\n",
      "Step [100 / 600], Loss: 0.08660593628883362\n",
      "Step [200 / 600], Loss: 0.04296163469552994\n",
      "Step [300 / 600], Loss: 0.1455627977848053\n",
      "Step [400 / 600], Loss: 0.13256700336933136\n",
      "Step [500 / 600], Loss: 0.11581713706254959\n",
      "Step [600 / 600], Loss: 0.036074671894311905\n",
      "TESTING...\n",
      "Loss: 1.0771445040144214, accuracy: 97.79%\n",
      "Started epoch: 6\n",
      "Step [100 / 600], Loss: 0.09012658149003983\n",
      "Step [200 / 600], Loss: 0.03038308583199978\n",
      "Step [300 / 600], Loss: 0.011829780414700508\n",
      "Step [400 / 600], Loss: 0.07780886441469193\n",
      "Step [500 / 600], Loss: 0.05151620879769325\n",
      "Step [600 / 600], Loss: 0.08912494778633118\n",
      "TESTING...\n",
      "Loss: 1.0689210643553213, accuracy: 98.13%\n",
      "Started epoch: 7\n",
      "Step [100 / 600], Loss: 0.05726093426346779\n",
      "Step [200 / 600], Loss: 0.02296975441277027\n",
      "Step [300 / 600], Loss: 0.07484350353479385\n",
      "Step [400 / 600], Loss: 0.023758970201015472\n",
      "Step [500 / 600], Loss: 0.12363723665475845\n",
      "Step [600 / 600], Loss: 0.015771254897117615\n",
      "TESTING...\n",
      "Loss: 1.0767247497038772, accuracy: 98.12%\n",
      "Started epoch: 8\n",
      "Step [100 / 600], Loss: 0.02041143737733364\n",
      "Step [200 / 600], Loss: 0.06328218430280685\n",
      "Step [300 / 600], Loss: 0.05640997737646103\n",
      "Step [400 / 600], Loss: 0.006682735867798328\n",
      "Step [500 / 600], Loss: 0.011643268167972565\n",
      "Step [600 / 600], Loss: 0.07634904235601425\n",
      "TESTING...\n",
      "Loss: 1.0613942008468173, accuracy: 98.45%\n",
      "Started epoch: 9\n",
      "Step [100 / 600], Loss: 0.1054324135184288\n",
      "Step [200 / 600], Loss: 0.003028307342901826\n",
      "Step [300 / 600], Loss: 0.04991855472326279\n",
      "Step [400 / 600], Loss: 0.008334452286362648\n",
      "Step [500 / 600], Loss: 0.04545349255204201\n",
      "Step [600 / 600], Loss: 0.2622760236263275\n",
      "TESTING...\n",
      "Loss: 1.0974741807834227, accuracy: 97.43%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train(num_epochs, model, loaders)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "isf_3",
   "language": "python",
   "name": "anaconda3_isf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "361px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
